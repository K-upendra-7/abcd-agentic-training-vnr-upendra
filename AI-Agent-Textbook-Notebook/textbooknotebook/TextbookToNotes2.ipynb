{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b586491",
   "metadata": {},
   "source": [
    "\n",
    "# AI Agent: Textbook to Notebook Notes Generator\n",
    " \n",
    "This AI agent processes textbook PDFs and converts them into concise, structured notes.\n",
    " The workflow involves:\n",
    " 1. Text extraction from PDF\n",
    " 2. Chunking for processing\n",
    " 3. Note generation using LLaMA3:8b\n",
    " 4. Quality evaluation using ChatGPT\n",
    " 5. PDF output creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c75052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 6a0746a1ec1a: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 4.7 GB                         \u001b[K\n",
      "pulling 4fa551d4f938: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  12 KB                         \u001b[K\n",
      "pulling 8ab4849b038c: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  254 B                         \u001b[K\n",
      "pulling 577073ffcc6c: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  110 B                         \u001b[K\n",
      "pulling 3f8eb4da87fa: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3:8b\n",
    "!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a2d5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "import textwrap\n",
    "import asyncio\n",
    "try:\n",
    "    import fitz  # PyMuPDF for PDF text extraction\n",
    "except Exception:\n",
    "    fitz = None\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel\n",
    "from fpdf import FPDF\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db756d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "# This is where API keys are stored securely\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97a3080f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key found, starting with: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "# Retrieve API keys from environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")  # For ChatGPT evaluation\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")  # Not used in this version\n",
    "\n",
    "# Check if required API keys are available\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key found, starting with: {openai_api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not found. Please set it in your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d843051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API clients for different AI models\n",
    "\n",
    "# LLaMA client - runs locally via Ollama\n",
    "ollama_client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Local Ollama server\n",
    "    api_key=\"ollama\"  # Dummy API key for local Ollama\n",
    ")\n",
    "\n",
    "# ChatGPT client - for evaluation (using free API)\n",
    "chatgpt_client = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=\"https://api.openai.com/v1\"  # Official OpenAI API endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e568d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(BaseModel):\n",
    "    \"\"\"Defines the structure for evaluation results from ChatGPT.\"\"\"\n",
    "    is_acceptable: bool  # Whether the notes meet quality standards\n",
    "    feedback: str       # Specific feedback for improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52f03658",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_notes(text_chunk: str, retries: int = 2, feedback: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate structured notes in PDF-friendly format for a given text chunk using LLaMA3:8b.\n",
    "    \n",
    "    Args:\n",
    "        text_chunk: The text to process and summarize\n",
    "        retries: Number of retry attempts if evaluation fails\n",
    "        feedback: Previous evaluation feedback for improvement\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated notes in markdown-like format\n",
    "    \"\"\"\n",
    "    \n",
    "    # System prompt to guide the AI in note generation\n",
    "    system_prompt = (\n",
    "        \"You are an expert academic assistant. \"\n",
    "        \"Produce clear, well-structured notes formatted for PDF output. \"\n",
    "        \"Use clear headings, bullet points, and concise summaries. \"\n",
    "        \"Focus on key ideas, definitions, and concepts from the provided text. \"\n",
    "        \"Format the content in a way that will look good when converted to PDF.\"\n",
    "    )\n",
    "\n",
    "    # Build user prompt with feedback if available (for retries)\n",
    "    if feedback:\n",
    "        user_prompt = (\n",
    "            f\"Improve the previous notes using this feedback:\\n{feedback}\\n\\n\"\n",
    "            f\"Original text:\\n{text_chunk}\"\n",
    "        )\n",
    "    else:\n",
    "        user_prompt = f\"Generate concise academic notes in PDF-friendly format for the following text:\\n{text_chunk}\"\n",
    "\n",
    "    # Prepare messages for the AI model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Call LLaMA model to generate notes\n",
    "        response = await ollama_client.chat.completions.create(\n",
    "            model=\"llama3:8b\",  # Local LLaMA model\n",
    "            messages=messages,\n",
    "        )\n",
    "        notes = response.choices[0].message.content\n",
    "        print(f\"‚úÖ Generated notes for chunk ({len(text_chunk)} chars)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ollama generation error: {e}\")\n",
    "        return f\"Error generating notes: {e}\"\n",
    "\n",
    "    # Evaluate notes and retry if needed\n",
    "    if retries > 0:\n",
    "        evaluation = await evaluate_notes(text_chunk, notes)\n",
    "        if not evaluation.is_acceptable:\n",
    "            print(f\"üîÅ Retrying with feedback: {evaluation.feedback}\")\n",
    "            return await generate_notes(text_chunk, retries - 1, evaluation.feedback)\n",
    "        else:\n",
    "            print(\"‚úÖ Evaluation passed.\")\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df606257",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_notes(text_chunk: str, notes: str) -> Evaluation:\n",
    "    \"\"\"\n",
    "    Evaluates generated notes using ChatGPT for accuracy and clarity.\n",
    "    \n",
    "    Args:\n",
    "        text_chunk: Original text that was processed\n",
    "        notes: Generated notes to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation: Object containing acceptability and feedback\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prompt for ChatGPT evaluation\n",
    "    prompt = (\n",
    "        \"You are a quality evaluator for academic notes. Check if:\\n\"\n",
    "        \"1. The notes accurately summarize the key points from the original text\\n\"\n",
    "        \"2. The notes are well-structured and organized\\n\"\n",
    "        \"3. The content is clear and easy to understand\\n\"\n",
    "        \"4. Important concepts, definitions, and ideas are captured\\n\\n\"\n",
    "        \"Respond in JSON format with these exact keys: 'is_acceptable' (boolean) and 'feedback' (string).\\n\\n\"\n",
    "        f\"--- Original Text ---\\n{text_chunk}\\n\\n\"\n",
    "        f\"--- Generated Notes ---\\n{notes}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Call ChatGPT for evaluation\n",
    "        response = await chatgpt_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Using free tier model\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"},  # Force JSON response\n",
    "        )\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        data = json.loads(response.choices[0].message.content)\n",
    "        evaluation = Evaluation(**data)\n",
    "        print(f\"‚úÖ Evaluation completed: {'PASS' if evaluation.is_acceptable else 'FAIL'}\")\n",
    "        return evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ChatGPT evaluation error: {e}\")\n",
    "        # Return acceptable by default if evaluation fails\n",
    "        return Evaluation(is_acceptable=True, feedback=f\"Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "683e5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_chars: int = 2500) -> list:\n",
    "    \"\"\"\n",
    "    Splits long text into smaller chunks for better processing by AI models.\n",
    "    \n",
    "    Args:\n",
    "        text: The full text to split into chunks\n",
    "        max_chars: Maximum characters per chunk\n",
    "        \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    return textwrap.wrap(text, width=max_chars, break_long_words=False, replace_whitespace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af27fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove or replace problematic Unicode characters that cause font issues in PDF.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text that may contain special characters\n",
    "        \n",
    "    Returns:\n",
    "        str: Sanitized text with ASCII equivalents\n",
    "    \"\"\"\n",
    "    # Replace common problematic Unicode characters with ASCII equivalents\n",
    "    replacements = {\n",
    "        '‚Ä¢': '-',      # bullet to hyphen\n",
    "        '‚Äì': '-',      # en dash to hyphen\n",
    "        '‚Äî': '-',      # em dash to hyphen\n",
    "        '‚Äú': '\"',      # left double quote\n",
    "        '‚Äù': '\"',      # right double quote\n",
    "        '‚Äò': \"'\",      # left single quote\n",
    "        '‚Äô': \"'\",      # right single quote\n",
    "        '‚Ä¶': '...',    # ellipsis\n",
    "        '‚Üí': '->',     # right arrow\n",
    "        '‚Üê': '<-',     # left arrow\n",
    "        '‚â•': '>=',     # greater than or equal\n",
    "        '‚â§': '<=',     # less than or equal\n",
    "        '√ó': 'x',      # multiplication sign\n",
    "        '√∑': '/',      # division sign\n",
    "        '¬±': '+/-',    # plus-minus\n",
    "    }\n",
    "    \n",
    "    for unicode_char, ascii_char in replacements.items():\n",
    "        text = text.replace(unicode_char, ascii_char)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fde488ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdf_file(notes_markdown: str, source_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert notes into a simple PDF with raw text formatting (no styling).\n",
    "    \n",
    "    Args:\n",
    "        notes_markdown: Generated notes in markdown-like format\n",
    "        source_filename: Original PDF filename for naming output\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the generated PDF file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output filename based on source\n",
    "    title = os.path.splitext(os.path.basename(source_filename))[0].replace('_', ' ').title()\n",
    "    output_filename = f\"{os.path.splitext(source_filename)[0]}_notes.pdf\"\n",
    "\n",
    "    # Create PDF with basic settings\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    \n",
    "    # Add a page\n",
    "    pdf.add_page()\n",
    "    \n",
    "    # Use only basic font (no styling)\n",
    "    pdf.set_font(\"Courier\", size=10)  # Monospace font for raw text\n",
    "    \n",
    "    # Add basic header information\n",
    "    pdf.cell(0, 10, f\"Academic Notes: {title}\", 0, 1)\n",
    "    pdf.cell(0, 8, f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M')}\", 0, 1)\n",
    "    pdf.ln(10)\n",
    "    \n",
    "    # Process the notes content as raw text\n",
    "    lines = notes_markdown.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # Sanitize text to remove problematic characters\n",
    "        safe_line = sanitize_text(line)\n",
    "        \n",
    "        # Simply add each line as text (no special formatting)\n",
    "        if safe_line.strip():  # Only add non-empty lines\n",
    "            # Use multi_cell for automatic line wrapping\n",
    "            pdf.multi_cell(0, 5, safe_line)\n",
    "        pdf.ln(2)  # Small spacing between lines\n",
    "    \n",
    "    # Add basic footer\n",
    "    pdf.set_y(-15)\n",
    "    pdf.set_font(\"Courier\", size=8)\n",
    "    pdf.cell(0, 10, f\"Source: {os.path.basename(source_filename)}\", 0, 0, 'C')\n",
    "    \n",
    "    try:\n",
    "        # Save the PDF file\n",
    "        pdf.output(output_filename)\n",
    "        print(f\"‚úÖ PDF created: {output_filename}\")\n",
    "        return output_filename\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è PDF creation error: {e}\")\n",
    "        # Create a simple text fallback if PDF fails\n",
    "        fallback = f\"{os.path.splitext(source_filename)[0]}_notes.txt\"\n",
    "        with open(fallback, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Notes for {title}\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
    "            f.write(notes_markdown)\n",
    "        print(f\"Saved fallback text file: {fallback}\")\n",
    "        return fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4913590",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_textbook(file, progress=gr.Progress()):\n",
    "    \"\"\"\n",
    "    Main processing pipeline: Extracts text, generates notes, evaluates quality, and creates PDF.\n",
    "    \n",
    "    Args:\n",
    "        file: Uploaded PDF file object\n",
    "        progress: Gradio progress tracker\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Status message and path to generated PDF\n",
    "    \"\"\"\n",
    "    if file is None:\n",
    "        return \"Please upload a textbook to begin.\", None\n",
    "\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "        \n",
    "        pdf_file_path = file.name\n",
    "        reader = PdfReader(pdf_file_path)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        # Step 1 ‚Äî Extract text from PDF\n",
    "        progress(0, desc=\"Step 1/4: Extracting text...\")\n",
    "        full_text = \"\"\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            progress((i + 1) / num_pages, desc=f\"Extracting Page {i + 1}/{num_pages}\")\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                full_text += page_text + \"\\n\"\n",
    "\n",
    "        if not full_text.strip():\n",
    "            return \"‚ö†Ô∏è No text extracted from the PDF.\", None\n",
    "\n",
    "        print(f\"‚úÖ Extracted {len(full_text)} characters from {num_pages} pages.\")\n",
    "\n",
    "        # Step 2 ‚Äî Chunk text and generate notes\n",
    "        chunks = chunk_text(full_text)\n",
    "        num_chunks = len(chunks)\n",
    "        all_notes = []\n",
    "\n",
    "        progress(0, desc=\"Step 2/4: Generating notes...\")\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            progress((i + 1) / num_chunks, desc=f\"Generating Chunk {i + 1}/{num_chunks}\")\n",
    "            notes_chunk = await generate_notes(chunk)\n",
    "            all_notes.append(notes_chunk)\n",
    "\n",
    "        # Combine all notes with separators\n",
    "        combined_notes = \"\\n\\n---\\n\\n\".join(all_notes)\n",
    "\n",
    "        # Step 3 ‚Äî Create PDF\n",
    "        progress(1, desc=\"Step 3/4: Creating PDF...\")\n",
    "        pdf_output_path = create_pdf_file(combined_notes, pdf_file_path)\n",
    "\n",
    "        # Step 4 ‚Äî Return results\n",
    "        if os.path.exists(pdf_output_path):\n",
    "            message = f\"‚úÖ Notes generated successfully!\\n\\n**Saved as:** {os.path.basename(pdf_output_path)}\"\n",
    "            return message, pdf_output_path\n",
    "        else:\n",
    "            return \"‚ùå PDF generation failed.\", None\n",
    "            \n",
    "    except ImportError:\n",
    "        return \"‚ùå PyPDF2 not installed. Please install it using: pip install PyPDF2\", None\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error processing PDF: {str(e)}\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e34a981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def create_notes_interface(file, progress=gr.Progress(track_tqdm=True)):\n",
    "    \"\"\"\n",
    "    Handles Gradio interface flow for the note generation process.\n",
    "    \n",
    "    Args:\n",
    "        file: Uploaded file from Gradio\n",
    "        progress: Progress tracking object\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Status message and file path\n",
    "    \"\"\"\n",
    "    if file is not None:\n",
    "        return await process_textbook(file, progress)\n",
    "    return \"Please upload a textbook.\", None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c80655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative PDF creation using reportlab (commented out since we're using raw text approach)\n",
    "def create_pdf_alternative(notes_markdown: str, source_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Alternative PDF creation method (not used in current implementation).\n",
    "    Kept as backup option.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from reportlab.lib.pagesizes import letter\n",
    "        from reportlab.pdfgen import canvas\n",
    "        # Implementation would go here...\n",
    "        return None\n",
    "    except ImportError:\n",
    "        print(\"ReportLab not available\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Alternative PDF creation failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0ced12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Textbook to Notes Generator...\n",
      "üìö Upload a PDF textbook to generate summarized notes\n",
      "* Running on local URL:  http://127.0.0.1:7889\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7889/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "        file_upload_statuses.is_tracked(upload_id), timeout=3\n",
      "    )\n",
      "  File \"C:\\Upendra\\Softwares\\Python313\\Lib\\asyncio\\tasks.py\", line 507, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Upendra\\Softwares\\Python313\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Upendra\\Softwares\\Python313\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x000001C78DDF5650 [unset]> is bound to a different event loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 19920 characters from 11 pages.\n",
      "‚úÖ Generated notes for chunk (2493 chars)\n",
      "‚ö†Ô∏è ChatGPT evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation passed.\n",
      "‚úÖ Generated notes for chunk (2492 chars)\n",
      "‚ö†Ô∏è ChatGPT evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation passed.\n",
      "‚úÖ Generated notes for chunk (2495 chars)\n",
      "‚ö†Ô∏è ChatGPT evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation passed.\n",
      "‚úÖ Generated notes for chunk (2498 chars)\n",
      "‚ö†Ô∏è ChatGPT evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation passed.\n",
      "‚úÖ Generated notes for chunk (2498 chars)\n",
      "‚ö†Ô∏è ChatGPT evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation passed.\n",
      "‚úÖ Generated notes for chunk (2495 chars)\n",
      "‚ö†Ô∏è ChatGPT evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation passed.\n",
      "‚úÖ Generated notes for chunk (2496 chars)\n",
      "‚ö†Ô∏è ChatGPT evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation passed.\n",
      "‚úÖ Generated notes for chunk (2443 chars)\n",
      "‚ö†Ô∏è ChatGPT evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation passed.\n",
      "‚úÖ PDF created: C:\\Users\\kolla\\AppData\\Local\\Temp\\gradio\\9d1f585023bbee10a956ef450312e39e0d4bec0d7c7c4dfda6190eb8846fa20d\\Textbook_notes.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kolla\\AppData\\Local\\Temp\\ipykernel_12012\\2492469319.py:28: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(0, 10, f\"Academic Notes: {title}\", 0, 1)\n",
      "C:\\Users\\kolla\\AppData\\Local\\Temp\\ipykernel_12012\\2492469319.py:29: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(0, 8, f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M')}\", 0, 1)\n",
      "C:\\Users\\kolla\\AppData\\Local\\Temp\\ipykernel_12012\\2492469319.py:48: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=0 use new_x=XPos.RIGHT, new_y=YPos.TOP.\n",
      "  pdf.cell(0, 10, f\"Source: {os.path.basename(source_filename)}\", 0, 0, 'C')\n"
     ]
    }
   ],
   "source": [
    "# Create Gradio web interface\n",
    "iface = gr.Interface(\n",
    "    fn=create_notes_interface,  # Main processing function\n",
    "    inputs=gr.File(label=\"üìò Upload Textbook (PDF)\"),  # File upload input\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"üßæ Status / Summary\"),  # Status display\n",
    "        gr.File(label=\"üì• Download Generated Notes (.pdf)\")  # Download output\n",
    "    ],\n",
    "    title=\"AI Textbook ‚Üí Notebook Notes Generator\",\n",
    "    description=(\n",
    "        \"Upload any textbook (PDF). The local LLaMA 3 (8B) model summarizes it into concise academic notes. \"\n",
    "        \"ChatGPT evaluates quality, and the output is saved as a raw text PDF.\"\n",
    "    ),\n",
    "    allow_flagging=\"never\"  # Disable Gradio's flagging feature\n",
    ")\n",
    "\n",
    "# Launch the application\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Textbook to Notes Generator...\")\n",
    "    print(\"üìö Upload a PDF textbook to generate summarized notes\")\n",
    "    iface.launch(server_name=\"127.0.0.1\", share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
