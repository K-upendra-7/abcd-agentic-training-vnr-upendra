{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c39332",
   "metadata": {},
   "source": [
    "\n",
    "# AI Agent: Textbook to Notebook Notes Generator\n",
    "This program converts a textbook PDF into concise, structured academic notes.\n",
    "It uses the local Llama3:8b model (via Ollama) to generate notes, and Gemini API to evaluate them.\n",
    "Finally, it creates a downloadable PDF using the FPDF library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80be6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "import textwrap\n",
    "import asyncio\n",
    "import markdown_it\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pypdf import PdfReader\n",
    "from pydantic import BaseModel\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0169801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from a .env file\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d39e7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 6a0746a1ec1a: 100% ▕██████████████████▏ 4.7 GB                         \u001b[K\n",
      "pulling 4fa551d4f938: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 8ab4849b038c: 100% ▕██████████████████▏  254 B                         \u001b[K\n",
      "pulling 577073ffcc6c: 100% ▕██████████████████▏  110 B                         \u001b[K\n",
      "pulling 3f8eb4da87fa: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.\n"
     ]
    }
   ],
   "source": [
    "# Download and start the Llama3 model server locally\n",
    "!ollama pull llama3:8b\n",
    "!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "110fe67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key found, starting with: sk-proj-...\n",
      "Google API Key found, starting with: AIzaSyDn...\n"
     ]
    }
   ],
   "source": [
    "# Fetch API keys for OpenAI (optional) and Google Gemini\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Display partial API keys for confirmation\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key found, starting with: {openai_api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not found. Please set it in your .env file.\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key found, starting with: {google_api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"Google API Key not found. Please set it in your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3a577ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create asynchronous clients for both Ollama (Llama3) and Gemini models\n",
    "ollama_client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "openai_client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "gemini_client = AsyncOpenAI(\n",
    "    api_key=google_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "078d14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data model for Gemini’s evaluation output\n",
    "class Evaluation(BaseModel):\n",
    "    \"\"\"Defines evaluator output.\"\"\"\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3fe94d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_notes(text_chunk: str, retries=2, feedback=\"\"):\n",
    "    \"\"\"\n",
    "    Generate structured notes for a given text chunk using Llama3:8b.\n",
    "    If Gemini evaluation fails, it retries with feedback for self-correction.\n",
    "    \"\"\"\n",
    "\n",
    "    # System prompt instructs the model to act as an academic assistant\n",
    "    system_prompt = (\n",
    "        \"You are an expert academic assistant. \"\n",
    "        \"Read the provided text and produce well-organized, clear Markdown notes. \"\n",
    "        \"Focus on key concepts, definitions, and main ideas. \"\n",
    "        \"Keep the language simple but precise.\"\n",
    "    )\n",
    "\n",
    "    # If previous output was rejected, include feedback for improvement\n",
    "    if feedback:\n",
    "        user_prompt = (\n",
    "            f\"The previous notes were not acceptable. Improve them using this feedback:\\n\"\n",
    "            f\"{feedback}\\n\\nOriginal Text:\\n{text_chunk}\"\n",
    "        )\n",
    "    else:\n",
    "        user_prompt = f\"Generate concise academic notes for the following text:\\n{text_chunk}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    # Try generating notes using the Ollama (Llama3) model\n",
    "    try:\n",
    "        response = await ollama_client.chat.completions.create(\n",
    "            model=\"llama3:8b\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        notes = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Ollama generation: {e}\")\n",
    "        return f\"Error generating notes: {e}\"\n",
    "\n",
    "    # Evaluate generated notes using Gemini\n",
    "    if retries > 0:\n",
    "        evaluation = await evaluate_notes(text_chunk, notes)\n",
    "        if not evaluation.is_acceptable:\n",
    "            print(f\"Evaluation failed. Retrying with feedback: {evaluation.feedback}\")\n",
    "            return await generate_notes(text_chunk, retries - 1, evaluation.feedback)\n",
    "        else:\n",
    "            print(\"Evaluation passed.\")\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01173334",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_notes(text_chunk: str, notes: str) -> Evaluation:\n",
    "    \"\"\"\n",
    "    Evaluates generated notes using ChatGPT-compatible free model from OpenRouter.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an evaluator. Rate the following notes based on accuracy, \"\n",
    "        \"clarity, and completeness. Return JSON only with keys:\\n\"\n",
    "        \"is_acceptable (boolean), feedback (string).\\n\\n\"\n",
    "        f\"--- Original Text ---\\n{text_chunk}\\n\\n\"\n",
    "        f\"--- Notes ---\\n{notes}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = await openai_client.chat.completions.create(\n",
    "            model=\"openai/gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        data = json.loads(content)\n",
    "        return Evaluation(**data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {e}\")\n",
    "        return Evaluation(is_acceptable=True, feedback=f\"Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "140a3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_chars: int = 2500):\n",
    "    \"\"\"\n",
    "    Splits long text into smaller chunks for easier processing.\n",
    "    Each chunk is roughly 'max_chars' characters long.\n",
    "    \"\"\"\n",
    "    return textwrap.wrap(text, width=max_chars, break_long_words=False, replace_whitespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7754890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdf_file(notes_markdown: str, source_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts generated Markdown notes into a formatted PDF using FPDF2.\n",
    "    Falls back to a Markdown file if PDF generation fails.\n",
    "    \"\"\"\n",
    "    title = os.path.splitext(os.path.basename(source_filename))[0].replace('_', ' ').title()\n",
    "    output_filename = f\"{os.path.splitext(source_filename)[0]}_notes.pdf\"\n",
    "    \n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "\n",
    "    # Add title\n",
    "    pdf.set_font(\"Arial\", \"B\", 18)\n",
    "    pdf.cell(0, 10, f\"Notes for {title}\", 0, 1, \"C\")\n",
    "    pdf.ln(10)\n",
    "\n",
    "    # Add body text (Markdown supported in FPDF2)\n",
    "    pdf.set_font(\"Arial\", \"\", 11)\n",
    "    \n",
    "    try:\n",
    "        pdf.write_markdown(notes_markdown)\n",
    "        pdf.output(output_filename)\n",
    "        print(f\"PDF created successfully: {output_filename}\")\n",
    "        return output_filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating PDF with fpdf2: {e}\")\n",
    "        md_filename = f\"{os.path.splitext(source_filename)[0]}_notes.md\"\n",
    "        with open(md_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# Notes for {title}\\n\\n{notes_markdown}\")\n",
    "        print(f\"As a fallback, Markdown file saved: {md_filename}\")\n",
    "        return md_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3f134907",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_textbook(file, progress=gr.Progress()):\n",
    "    \"\"\"\n",
    "    Main orchestrator:\n",
    "    1. Extracts all text from the uploaded PDF.\n",
    "    2. Splits it into chunks.\n",
    "    3. Generates notes for each chunk using Llama3.\n",
    "    4. Evaluates quality using Gemini.\n",
    "    5. Combines all notes and saves as a PDF.\n",
    "    \"\"\"\n",
    "    if file is None:\n",
    "        return None\n",
    "\n",
    "    pdf_file_path = file.name\n",
    "    reader = PdfReader(pdf_file_path)\n",
    "    num_pages = len(reader.pages)\n",
    "    \n",
    "    print(f\"Extracting text from {num_pages} pages...\")\n",
    "    progress(0, desc=\"Step 1/3: Extracting Text...\")\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract text page by page\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        progress((i + 1) / num_pages, desc=f\"Extracting from Page {i + 1}/{num_pages}\")\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text += page_text + \"\\n\"\n",
    "\n",
    "    if not full_text.strip():\n",
    "        print(\"No text could be extracted from the PDF.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Text extraction complete. Total characters: {len(full_text)}\")\n",
    "    \n",
    "    # Split text and process in chunks\n",
    "    chunks = chunk_text(full_text)\n",
    "    num_chunks = len(chunks)\n",
    "    all_notes = []\n",
    "    \n",
    "    print(f\"Generating notes from {num_chunks} text chunks...\")\n",
    "    progress(0, desc=\"Step 2/3: Generating Notes...\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        progress((i + 1) / num_chunks, desc=f\"Processing Chunk {i + 1}/{num_chunks}\")\n",
    "        notes_chunk = await generate_notes(chunk)\n",
    "        all_notes.append(notes_chunk)\n",
    "        \n",
    "    combined_notes = \"\\n\\n---\\n\\n\".join(all_notes)\n",
    "\n",
    "    progress(1, desc=\"Step 3/3: Creating PDF...\")\n",
    "    pdf_path = create_pdf_file(combined_notes, pdf_file_path)\n",
    "    return pdf_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e8386c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7888\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7888/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "        file_upload_statuses.is_tracked(upload_id), timeout=3\n",
      "    )\n",
      "  File \"C:\\Upendra\\Softwares\\Python313\\Lib\\asyncio\\tasks.py\", line 507, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Upendra\\Git Hub\\Git Hub -- K-Upendra-7\\abcd-agentic-training-vnr-upendra\\AI-Agent-Textbook-Notebook\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Upendra\\Softwares\\Python313\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Upendra\\Softwares\\Python313\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x000002091CC97D10 [unset]> is bound to a different event loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from 11 pages...\n",
      "Text extraction complete. Total characters: 19898\n",
      "Generating notes from 8 text chunks...\n",
      "Evaluation error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
      "Evaluation passed.\n",
      "Evaluation error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
      "Evaluation passed.\n",
      "Evaluation error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
      "Evaluation passed.\n",
      "Evaluation error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
      "Evaluation passed.\n",
      "Evaluation error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
      "Evaluation passed.\n",
      "Evaluation error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
      "Evaluation passed.\n",
      "Evaluation error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
      "Evaluation passed.\n",
      "Evaluation error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
      "Evaluation passed.\n",
      "Error creating PDF with fpdf2: 'FPDF' object has no attribute 'write_markdown'\n",
      "As a fallback, Markdown file saved: C:\\Users\\kolla\\AppData\\Local\\Temp\\gradio\\9d1f585023bbee10a956ef450312e39e0d4bec0d7c7c4dfda6190eb8846fa20d\\Textbook_notes.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kolla\\AppData\\Local\\Temp\\ipykernel_20056\\865379566.py:13: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", \"B\", 18)\n",
      "C:\\Users\\kolla\\AppData\\Local\\Temp\\ipykernel_20056\\865379566.py:14: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(0, 10, f\"Notes for {title}\", 0, 1, \"C\")\n",
      "C:\\Users\\kolla\\AppData\\Local\\Temp\\ipykernel_20056\\865379566.py:18: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", \"\", 11)\n"
     ]
    }
   ],
   "source": [
    "async def create_notes_interface(file, progress=gr.Progress(track_tqdm=True)):\n",
    "    \"\"\"\n",
    "    Wrapper for Gradio interface to handle file upload and call the main process.\n",
    "    \"\"\"\n",
    "    if file is not None:\n",
    "        return await process_textbook(file, progress)\n",
    "    return \"Please upload a textbook to begin.\"\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=create_notes_interface,\n",
    "    inputs=gr.File(label=\"Upload Textbook (PDF)\"),\n",
    "    outputs=gr.File(label=\"Download Generated Notes (.pdf)\"),\n",
    "    title=\"AI Textbook → PDF Notes Generator\",\n",
    "    description=(\n",
    "        \"Upload a textbook in PDF format. The local llama3:8b model generates a cohesive summary \"\n",
    "        \"of the entire book, Gemini API evaluates its quality, and you receive a downloadable PDF.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(server_name=\"127.0.0.1\", share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
