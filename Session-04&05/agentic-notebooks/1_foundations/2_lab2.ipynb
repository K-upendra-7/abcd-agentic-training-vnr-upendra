{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key not set (and this is optional)\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY') # paid\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY') # upfront 5$ or such\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY') # free\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY') # 2$\n",
    "groq_api_key = os.getenv('GROQ_API_KEY') # pay only for what u use\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you were tasked with designing an ethical framework for autonomous AI decision-making in life-or-death situations, what key principles would you prioritize and how would you reconcile potential conflicts between them?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing an ethical framework for autonomous AI decision-making in life-or-death situations is a complex and nuanced task that demands careful consideration of various ethical principles. Here are key principles that I would prioritize, along with strategies for reconciling potential conflicts between them:\n",
       "\n",
       "### Key Principles\n",
       "\n",
       "1. **Human Life Preservation**:\n",
       "   - Priority should be given to preserving human life whenever possible. The AI should make decisions that minimize harm and prioritize saving lives.\n",
       "\n",
       "2. **Autonomy**:\n",
       "   - Respect for individual autonomy is crucial. This includes recognizing the ability of individuals to make their own choices about their lives, including end-of-life decisions.\n",
       "\n",
       "3. **Justice**:\n",
       "   - Decisions should be made fairly, without bias or discrimination. The AI should ensure equitable treatment of all individuals involved, irrespective of their background, status, or other characteristics.\n",
       "\n",
       "4. **Transparency**:\n",
       "   - The decision-making process of the AI should be transparent to stakeholders, enabling accountability and understanding of the rationale behind decisions.\n",
       "\n",
       "5. **Accountability**:\n",
       "   - There should be mechanisms for accountability that determine who is responsible for the AI's actions, especially in critical scenarios where lives are at stake.\n",
       "\n",
       "6. **Minimization of Harm**:\n",
       "   - The principle of non-maleficence emphasizes the need to minimize harm to individuals and society while maximizing benefits.\n",
       "\n",
       "7. **Utility**:\n",
       "   - Decisions should consider overall utility, aiming for the greatest good for the greatest number while balancing the needs of individuals.\n",
       "\n",
       "### Reconciling Potential Conflicts\n",
       "\n",
       "1. **Conflicts Between Human Life Preservation and Autonomy**:\n",
       "   - **Resolution**: Incorporate value-sensitive design that respects patient or individual preferences (such as advance directives) while still prioritizing life. Establish policies that allow AI to refer complex cases back to human decision-makers when conflicts arise.\n",
       "\n",
       "2. **Conflicts Between Justice and Utility**:\n",
       "   - **Resolution**: Use a weighted system for decision-making that factors in both equitable treatment and overall utility. Ensure that the data used to inform decisions are representative to avoid biases that skew towards utility at the expense of justice.\n",
       "\n",
       "3. **Conflicts Between Transparency and Accountability**:\n",
       "   - **Resolution**: Develop a layered transparency model, where high-level decisions are clear while proprietary algorithms or trade secrets remain protected. Conduct regular audits and peer reviews to ensure accountability.\n",
       "\n",
       "4. **Conflicts Between Minimization of Harm and Utility**:\n",
       "   - **Resolution**: Utilize an ethical multi-criteria decision analysis (MCDA) approach, where trade-offs are evaluated carefully to discern situations where minimizing harm may lean more heavily than achieving utility, ensuring safeguards against oversights.\n",
       "\n",
       "5. **Conflicts in Accountability**:\n",
       "   - **Resolution**: Establish a clear framework defining accountability at multiple levels (e.g., developers, operators, organizations). Create legal and ethical guidelines that adapt to evolving AI technologies while protecting human rights.\n",
       "\n",
       "### Implementation Considerations\n",
       "\n",
       "- **Stakeholder Involvement**: Include diverse stakeholders in the design and continuous reevaluation of the ethical framework, ensuring representation from ethicists, legal experts, healthcare professionals, and affected communities.\n",
       "- **Adaptive Policy**: The framework should be flexible enough to adapt to new ethical dilemmas presented by advances in AI and changing societal values.\n",
       "- **Training and Education**: Ensure that developers and operators of AI systems are educated on ethical principles and dilemmas related to life-or-death decision-making.\n",
       "\n",
       "This ethical framework would lay the groundwork for thoughtful, responsible AI decision-making in critical situations while prioritizing accountability and societal trust in the technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing an ethical framework for autonomous AI decision-making in life-or-death situations requires careful consideration of various moral principles. Here's a proposed framework that prioritizes the following key principles:\n",
       "\n",
       "1. **Principle of Benefit**: The primary goal is to maximize overall well-being, safety, and survival benefits for individuals and society as a whole.\n",
       "2. **Principle of Non-Maleficence**: Minimize harm and avoid causing unnecessary suffering or injury to any individual or group.\n",
       "3. **Principle of Autonomy**: Respect the autonomy of individuals and allow them to make decisions about their own lives, whenever possible.\n",
       "4. **Principle of Fairness**: Ensure that AI decision-making is fair, transparent, and unbiased, avoiding discrimination and prejudice based on factors like race, gender, age, or social status.\n",
       "5. **Principle of Responsibility**: Hold entities accountable for the consequences of their actions, including AI systems.\n",
       "\n",
       "To reconcile potential conflicts between these principles, consider the following approaches:\n",
       "\n",
       "1. **Gradation of Principles**: Introduce a hierarchical structure to prioritize these principles in different situations, with flexibility to adjust based on context and specific circumstances.\n",
       "2. **Multi-Objective Optimization**: Use machine learning algorithms that optimize multiple objectives simultaneously, allowing AI systems to balance competing values in dynamic environments.\n",
       "3. **Value Alignment**: Incorporate human values into the AI system's decision-making process through value alignment techniques, such as attention-weighted networks, which can help minimize harm and maximize benefits.\n",
       "4. **Regulatory Oversight**: Establish regulatory frameworks that establish clear guidelines for AI development and deployment, ensuring accountability for decisions made by autonomous systems.\n",
       "5. **Transparency and Explainability**: Implement transparent decision-making processes, using techniques like model interpretability or saliency maps to provide insights into the reasoning behind AI-driven decisions.\n",
       "\n",
       "To ensure these principles are consistently applied\n",
       "\n",
       "1.  Establish a clear set of objectives that reflect human values and societal norms.\n",
       "2.  Use value-based frameworks and ontologies for structuring and prioritizing decision-making processes in AI applications.\n",
       "3.  Incorporate diverse perspectives from experts like ethicists, philosophers, and domain experts to develop robust ethical guidelines.\n",
       "\n",
       "These principles can be applied to a range of situations involving autonomous AI, such as:\n",
       "\n",
       "1. **Medical Diagnosis**: Ensure AI systems provide accurate diagnoses that prioritize patient safety, while respecting patient autonomy in informed consent decisions.\n",
       "2.  Autonomous Driving"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'llama3.2']\n",
      "[\"Designing an ethical framework for autonomous AI decision-making in life-or-death situations is a complex and nuanced task that demands careful consideration of various ethical principles. Here are key principles that I would prioritize, along with strategies for reconciling potential conflicts between them:\\n\\n### Key Principles\\n\\n1. **Human Life Preservation**:\\n   - Priority should be given to preserving human life whenever possible. The AI should make decisions that minimize harm and prioritize saving lives.\\n\\n2. **Autonomy**:\\n   - Respect for individual autonomy is crucial. This includes recognizing the ability of individuals to make their own choices about their lives, including end-of-life decisions.\\n\\n3. **Justice**:\\n   - Decisions should be made fairly, without bias or discrimination. The AI should ensure equitable treatment of all individuals involved, irrespective of their background, status, or other characteristics.\\n\\n4. **Transparency**:\\n   - The decision-making process of the AI should be transparent to stakeholders, enabling accountability and understanding of the rationale behind decisions.\\n\\n5. **Accountability**:\\n   - There should be mechanisms for accountability that determine who is responsible for the AI's actions, especially in critical scenarios where lives are at stake.\\n\\n6. **Minimization of Harm**:\\n   - The principle of non-maleficence emphasizes the need to minimize harm to individuals and society while maximizing benefits.\\n\\n7. **Utility**:\\n   - Decisions should consider overall utility, aiming for the greatest good for the greatest number while balancing the needs of individuals.\\n\\n### Reconciling Potential Conflicts\\n\\n1. **Conflicts Between Human Life Preservation and Autonomy**:\\n   - **Resolution**: Incorporate value-sensitive design that respects patient or individual preferences (such as advance directives) while still prioritizing life. Establish policies that allow AI to refer complex cases back to human decision-makers when conflicts arise.\\n\\n2. **Conflicts Between Justice and Utility**:\\n   - **Resolution**: Use a weighted system for decision-making that factors in both equitable treatment and overall utility. Ensure that the data used to inform decisions are representative to avoid biases that skew towards utility at the expense of justice.\\n\\n3. **Conflicts Between Transparency and Accountability**:\\n   - **Resolution**: Develop a layered transparency model, where high-level decisions are clear while proprietary algorithms or trade secrets remain protected. Conduct regular audits and peer reviews to ensure accountability.\\n\\n4. **Conflicts Between Minimization of Harm and Utility**:\\n   - **Resolution**: Utilize an ethical multi-criteria decision analysis (MCDA) approach, where trade-offs are evaluated carefully to discern situations where minimizing harm may lean more heavily than achieving utility, ensuring safeguards against oversights.\\n\\n5. **Conflicts in Accountability**:\\n   - **Resolution**: Establish a clear framework defining accountability at multiple levels (e.g., developers, operators, organizations). Create legal and ethical guidelines that adapt to evolving AI technologies while protecting human rights.\\n\\n### Implementation Considerations\\n\\n- **Stakeholder Involvement**: Include diverse stakeholders in the design and continuous reevaluation of the ethical framework, ensuring representation from ethicists, legal experts, healthcare professionals, and affected communities.\\n- **Adaptive Policy**: The framework should be flexible enough to adapt to new ethical dilemmas presented by advances in AI and changing societal values.\\n- **Training and Education**: Ensure that developers and operators of AI systems are educated on ethical principles and dilemmas related to life-or-death decision-making.\\n\\nThis ethical framework would lay the groundwork for thoughtful, responsible AI decision-making in critical situations while prioritizing accountability and societal trust in the technology.\", \"Designing an ethical framework for autonomous AI decision-making in life-or-death situations requires careful consideration of various moral principles. Here's a proposed framework that prioritizes the following key principles:\\n\\n1. **Principle of Benefit**: The primary goal is to maximize overall well-being, safety, and survival benefits for individuals and society as a whole.\\n2. **Principle of Non-Maleficence**: Minimize harm and avoid causing unnecessary suffering or injury to any individual or group.\\n3. **Principle of Autonomy**: Respect the autonomy of individuals and allow them to make decisions about their own lives, whenever possible.\\n4. **Principle of Fairness**: Ensure that AI decision-making is fair, transparent, and unbiased, avoiding discrimination and prejudice based on factors like race, gender, age, or social status.\\n5. **Principle of Responsibility**: Hold entities accountable for the consequences of their actions, including AI systems.\\n\\nTo reconcile potential conflicts between these principles, consider the following approaches:\\n\\n1. **Gradation of Principles**: Introduce a hierarchical structure to prioritize these principles in different situations, with flexibility to adjust based on context and specific circumstances.\\n2. **Multi-Objective Optimization**: Use machine learning algorithms that optimize multiple objectives simultaneously, allowing AI systems to balance competing values in dynamic environments.\\n3. **Value Alignment**: Incorporate human values into the AI system's decision-making process through value alignment techniques, such as attention-weighted networks, which can help minimize harm and maximize benefits.\\n4. **Regulatory Oversight**: Establish regulatory frameworks that establish clear guidelines for AI development and deployment, ensuring accountability for decisions made by autonomous systems.\\n5. **Transparency and Explainability**: Implement transparent decision-making processes, using techniques like model interpretability or saliency maps to provide insights into the reasoning behind AI-driven decisions.\\n\\nTo ensure these principles are consistently applied\\n\\n1.  Establish a clear set of objectives that reflect human values and societal norms.\\n2.  Use value-based frameworks and ontologies for structuring and prioritizing decision-making processes in AI applications.\\n3.  Incorporate diverse perspectives from experts like ethicists, philosophers, and domain experts to develop robust ethical guidelines.\\n\\nThese principles can be applied to a range of situations involving autonomous AI, such as:\\n\\n1. **Medical Diagnosis**: Ensure AI systems provide accurate diagnoses that prioritize patient safety, while respecting patient autonomy in informed consent decisions.\\n2.  Autonomous Driving\"]\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gpt-4o-mini\n",
      "\n",
      "Designing an ethical framework for autonomous AI decision-making in life-or-death situations is a complex and nuanced task that demands careful consideration of various ethical principles. Here are key principles that I would prioritize, along with strategies for reconciling potential conflicts between them:\n",
      "\n",
      "### Key Principles\n",
      "\n",
      "1. **Human Life Preservation**:\n",
      "   - Priority should be given to preserving human life whenever possible. The AI should make decisions that minimize harm and prioritize saving lives.\n",
      "\n",
      "2. **Autonomy**:\n",
      "   - Respect for individual autonomy is crucial. This includes recognizing the ability of individuals to make their own choices about their lives, including end-of-life decisions.\n",
      "\n",
      "3. **Justice**:\n",
      "   - Decisions should be made fairly, without bias or discrimination. The AI should ensure equitable treatment of all individuals involved, irrespective of their background, status, or other characteristics.\n",
      "\n",
      "4. **Transparency**:\n",
      "   - The decision-making process of the AI should be transparent to stakeholders, enabling accountability and understanding of the rationale behind decisions.\n",
      "\n",
      "5. **Accountability**:\n",
      "   - There should be mechanisms for accountability that determine who is responsible for the AI's actions, especially in critical scenarios where lives are at stake.\n",
      "\n",
      "6. **Minimization of Harm**:\n",
      "   - The principle of non-maleficence emphasizes the need to minimize harm to individuals and society while maximizing benefits.\n",
      "\n",
      "7. **Utility**:\n",
      "   - Decisions should consider overall utility, aiming for the greatest good for the greatest number while balancing the needs of individuals.\n",
      "\n",
      "### Reconciling Potential Conflicts\n",
      "\n",
      "1. **Conflicts Between Human Life Preservation and Autonomy**:\n",
      "   - **Resolution**: Incorporate value-sensitive design that respects patient or individual preferences (such as advance directives) while still prioritizing life. Establish policies that allow AI to refer complex cases back to human decision-makers when conflicts arise.\n",
      "\n",
      "2. **Conflicts Between Justice and Utility**:\n",
      "   - **Resolution**: Use a weighted system for decision-making that factors in both equitable treatment and overall utility. Ensure that the data used to inform decisions are representative to avoid biases that skew towards utility at the expense of justice.\n",
      "\n",
      "3. **Conflicts Between Transparency and Accountability**:\n",
      "   - **Resolution**: Develop a layered transparency model, where high-level decisions are clear while proprietary algorithms or trade secrets remain protected. Conduct regular audits and peer reviews to ensure accountability.\n",
      "\n",
      "4. **Conflicts Between Minimization of Harm and Utility**:\n",
      "   - **Resolution**: Utilize an ethical multi-criteria decision analysis (MCDA) approach, where trade-offs are evaluated carefully to discern situations where minimizing harm may lean more heavily than achieving utility, ensuring safeguards against oversights.\n",
      "\n",
      "5. **Conflicts in Accountability**:\n",
      "   - **Resolution**: Establish a clear framework defining accountability at multiple levels (e.g., developers, operators, organizations). Create legal and ethical guidelines that adapt to evolving AI technologies while protecting human rights.\n",
      "\n",
      "### Implementation Considerations\n",
      "\n",
      "- **Stakeholder Involvement**: Include diverse stakeholders in the design and continuous reevaluation of the ethical framework, ensuring representation from ethicists, legal experts, healthcare professionals, and affected communities.\n",
      "- **Adaptive Policy**: The framework should be flexible enough to adapt to new ethical dilemmas presented by advances in AI and changing societal values.\n",
      "- **Training and Education**: Ensure that developers and operators of AI systems are educated on ethical principles and dilemmas related to life-or-death decision-making.\n",
      "\n",
      "This ethical framework would lay the groundwork for thoughtful, responsible AI decision-making in critical situations while prioritizing accountability and societal trust in the technology.\n",
      "Competitor: llama3.2\n",
      "\n",
      "Designing an ethical framework for autonomous AI decision-making in life-or-death situations requires careful consideration of various moral principles. Here's a proposed framework that prioritizes the following key principles:\n",
      "\n",
      "1. **Principle of Benefit**: The primary goal is to maximize overall well-being, safety, and survival benefits for individuals and society as a whole.\n",
      "2. **Principle of Non-Maleficence**: Minimize harm and avoid causing unnecessary suffering or injury to any individual or group.\n",
      "3. **Principle of Autonomy**: Respect the autonomy of individuals and allow them to make decisions about their own lives, whenever possible.\n",
      "4. **Principle of Fairness**: Ensure that AI decision-making is fair, transparent, and unbiased, avoiding discrimination and prejudice based on factors like race, gender, age, or social status.\n",
      "5. **Principle of Responsibility**: Hold entities accountable for the consequences of their actions, including AI systems.\n",
      "\n",
      "To reconcile potential conflicts between these principles, consider the following approaches:\n",
      "\n",
      "1. **Gradation of Principles**: Introduce a hierarchical structure to prioritize these principles in different situations, with flexibility to adjust based on context and specific circumstances.\n",
      "2. **Multi-Objective Optimization**: Use machine learning algorithms that optimize multiple objectives simultaneously, allowing AI systems to balance competing values in dynamic environments.\n",
      "3. **Value Alignment**: Incorporate human values into the AI system's decision-making process through value alignment techniques, such as attention-weighted networks, which can help minimize harm and maximize benefits.\n",
      "4. **Regulatory Oversight**: Establish regulatory frameworks that establish clear guidelines for AI development and deployment, ensuring accountability for decisions made by autonomous systems.\n",
      "5. **Transparency and Explainability**: Implement transparent decision-making processes, using techniques like model interpretability or saliency maps to provide insights into the reasoning behind AI-driven decisions.\n",
      "\n",
      "To ensure these principles are consistently applied\n",
      "\n",
      "1.  Establish a clear set of objectives that reflect human values and societal norms.\n",
      "2.  Use value-based frameworks and ontologies for structuring and prioritizing decision-making processes in AI applications.\n",
      "3.  Incorporate diverse perspectives from experts like ethicists, philosophers, and domain experts to develop robust ethical guidelines.\n",
      "\n",
      "These principles can be applied to a range of situations involving autonomous AI, such as:\n",
      "\n",
      "1. **Medical Diagnosis**: Ensure AI systems provide accurate diagnoses that prioritize patient safety, while respecting patient autonomy in informed consent decisions.\n",
      "2.  Autonomous Driving\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Designing an ethical framework for autonomous AI decision-making in life-or-death situations is a complex and nuanced task that demands careful consideration of various ethical principles. Here are key principles that I would prioritize, along with strategies for reconciling potential conflicts between them:\n",
      "\n",
      "### Key Principles\n",
      "\n",
      "1. **Human Life Preservation**:\n",
      "   - Priority should be given to preserving human life whenever possible. The AI should make decisions that minimize harm and prioritize saving lives.\n",
      "\n",
      "2. **Autonomy**:\n",
      "   - Respect for individual autonomy is crucial. This includes recognizing the ability of individuals to make their own choices about their lives, including end-of-life decisions.\n",
      "\n",
      "3. **Justice**:\n",
      "   - Decisions should be made fairly, without bias or discrimination. The AI should ensure equitable treatment of all individuals involved, irrespective of their background, status, or other characteristics.\n",
      "\n",
      "4. **Transparency**:\n",
      "   - The decision-making process of the AI should be transparent to stakeholders, enabling accountability and understanding of the rationale behind decisions.\n",
      "\n",
      "5. **Accountability**:\n",
      "   - There should be mechanisms for accountability that determine who is responsible for the AI's actions, especially in critical scenarios where lives are at stake.\n",
      "\n",
      "6. **Minimization of Harm**:\n",
      "   - The principle of non-maleficence emphasizes the need to minimize harm to individuals and society while maximizing benefits.\n",
      "\n",
      "7. **Utility**:\n",
      "   - Decisions should consider overall utility, aiming for the greatest good for the greatest number while balancing the needs of individuals.\n",
      "\n",
      "### Reconciling Potential Conflicts\n",
      "\n",
      "1. **Conflicts Between Human Life Preservation and Autonomy**:\n",
      "   - **Resolution**: Incorporate value-sensitive design that respects patient or individual preferences (such as advance directives) while still prioritizing life. Establish policies that allow AI to refer complex cases back to human decision-makers when conflicts arise.\n",
      "\n",
      "2. **Conflicts Between Justice and Utility**:\n",
      "   - **Resolution**: Use a weighted system for decision-making that factors in both equitable treatment and overall utility. Ensure that the data used to inform decisions are representative to avoid biases that skew towards utility at the expense of justice.\n",
      "\n",
      "3. **Conflicts Between Transparency and Accountability**:\n",
      "   - **Resolution**: Develop a layered transparency model, where high-level decisions are clear while proprietary algorithms or trade secrets remain protected. Conduct regular audits and peer reviews to ensure accountability.\n",
      "\n",
      "4. **Conflicts Between Minimization of Harm and Utility**:\n",
      "   - **Resolution**: Utilize an ethical multi-criteria decision analysis (MCDA) approach, where trade-offs are evaluated carefully to discern situations where minimizing harm may lean more heavily than achieving utility, ensuring safeguards against oversights.\n",
      "\n",
      "5. **Conflicts in Accountability**:\n",
      "   - **Resolution**: Establish a clear framework defining accountability at multiple levels (e.g., developers, operators, organizations). Create legal and ethical guidelines that adapt to evolving AI technologies while protecting human rights.\n",
      "\n",
      "### Implementation Considerations\n",
      "\n",
      "- **Stakeholder Involvement**: Include diverse stakeholders in the design and continuous reevaluation of the ethical framework, ensuring representation from ethicists, legal experts, healthcare professionals, and affected communities.\n",
      "- **Adaptive Policy**: The framework should be flexible enough to adapt to new ethical dilemmas presented by advances in AI and changing societal values.\n",
      "- **Training and Education**: Ensure that developers and operators of AI systems are educated on ethical principles and dilemmas related to life-or-death decision-making.\n",
      "\n",
      "This ethical framework would lay the groundwork for thoughtful, responsible AI decision-making in critical situations while prioritizing accountability and societal trust in the technology.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "Designing an ethical framework for autonomous AI decision-making in life-or-death situations requires careful consideration of various moral principles. Here's a proposed framework that prioritizes the following key principles:\n",
      "\n",
      "1. **Principle of Benefit**: The primary goal is to maximize overall well-being, safety, and survival benefits for individuals and society as a whole.\n",
      "2. **Principle of Non-Maleficence**: Minimize harm and avoid causing unnecessary suffering or injury to any individual or group.\n",
      "3. **Principle of Autonomy**: Respect the autonomy of individuals and allow them to make decisions about their own lives, whenever possible.\n",
      "4. **Principle of Fairness**: Ensure that AI decision-making is fair, transparent, and unbiased, avoiding discrimination and prejudice based on factors like race, gender, age, or social status.\n",
      "5. **Principle of Responsibility**: Hold entities accountable for the consequences of their actions, including AI systems.\n",
      "\n",
      "To reconcile potential conflicts between these principles, consider the following approaches:\n",
      "\n",
      "1. **Gradation of Principles**: Introduce a hierarchical structure to prioritize these principles in different situations, with flexibility to adjust based on context and specific circumstances.\n",
      "2. **Multi-Objective Optimization**: Use machine learning algorithms that optimize multiple objectives simultaneously, allowing AI systems to balance competing values in dynamic environments.\n",
      "3. **Value Alignment**: Incorporate human values into the AI system's decision-making process through value alignment techniques, such as attention-weighted networks, which can help minimize harm and maximize benefits.\n",
      "4. **Regulatory Oversight**: Establish regulatory frameworks that establish clear guidelines for AI development and deployment, ensuring accountability for decisions made by autonomous systems.\n",
      "5. **Transparency and Explainability**: Implement transparent decision-making processes, using techniques like model interpretability or saliency maps to provide insights into the reasoning behind AI-driven decisions.\n",
      "\n",
      "To ensure these principles are consistently applied\n",
      "\n",
      "1.  Establish a clear set of objectives that reflect human values and societal norms.\n",
      "2.  Use value-based frameworks and ontologies for structuring and prioritizing decision-making processes in AI applications.\n",
      "3.  Incorporate diverse perspectives from experts like ethicists, philosophers, and domain experts to develop robust ethical guidelines.\n",
      "\n",
      "These principles can be applied to a range of situations involving autonomous AI, such as:\n",
      "\n",
      "1. **Medical Diagnosis**: Ensure AI systems provide accurate diagnoses that prioritize patient safety, while respecting patient autonomy in informed consent decisions.\n",
      "2.  Autonomous Driving\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 2 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "If you were tasked with designing an ethical framework for autonomous AI decision-making in life-or-death situations, what key principles would you prioritize and how would you reconcile potential conflicts between them?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "Designing an ethical framework for autonomous AI decision-making in life-or-death situations is a complex and nuanced task that demands careful consideration of various ethical principles. Here are key principles that I would prioritize, along with strategies for reconciling potential conflicts between them:\n",
      "\n",
      "### Key Principles\n",
      "\n",
      "1. **Human Life Preservation**:\n",
      "   - Priority should be given to preserving human life whenever possible. The AI should make decisions that minimize harm and prioritize saving lives.\n",
      "\n",
      "2. **Autonomy**:\n",
      "   - Respect for individual autonomy is crucial. This includes recognizing the ability of individuals to make their own choices about their lives, including end-of-life decisions.\n",
      "\n",
      "3. **Justice**:\n",
      "   - Decisions should be made fairly, without bias or discrimination. The AI should ensure equitable treatment of all individuals involved, irrespective of their background, status, or other characteristics.\n",
      "\n",
      "4. **Transparency**:\n",
      "   - The decision-making process of the AI should be transparent to stakeholders, enabling accountability and understanding of the rationale behind decisions.\n",
      "\n",
      "5. **Accountability**:\n",
      "   - There should be mechanisms for accountability that determine who is responsible for the AI's actions, especially in critical scenarios where lives are at stake.\n",
      "\n",
      "6. **Minimization of Harm**:\n",
      "   - The principle of non-maleficence emphasizes the need to minimize harm to individuals and society while maximizing benefits.\n",
      "\n",
      "7. **Utility**:\n",
      "   - Decisions should consider overall utility, aiming for the greatest good for the greatest number while balancing the needs of individuals.\n",
      "\n",
      "### Reconciling Potential Conflicts\n",
      "\n",
      "1. **Conflicts Between Human Life Preservation and Autonomy**:\n",
      "   - **Resolution**: Incorporate value-sensitive design that respects patient or individual preferences (such as advance directives) while still prioritizing life. Establish policies that allow AI to refer complex cases back to human decision-makers when conflicts arise.\n",
      "\n",
      "2. **Conflicts Between Justice and Utility**:\n",
      "   - **Resolution**: Use a weighted system for decision-making that factors in both equitable treatment and overall utility. Ensure that the data used to inform decisions are representative to avoid biases that skew towards utility at the expense of justice.\n",
      "\n",
      "3. **Conflicts Between Transparency and Accountability**:\n",
      "   - **Resolution**: Develop a layered transparency model, where high-level decisions are clear while proprietary algorithms or trade secrets remain protected. Conduct regular audits and peer reviews to ensure accountability.\n",
      "\n",
      "4. **Conflicts Between Minimization of Harm and Utility**:\n",
      "   - **Resolution**: Utilize an ethical multi-criteria decision analysis (MCDA) approach, where trade-offs are evaluated carefully to discern situations where minimizing harm may lean more heavily than achieving utility, ensuring safeguards against oversights.\n",
      "\n",
      "5. **Conflicts in Accountability**:\n",
      "   - **Resolution**: Establish a clear framework defining accountability at multiple levels (e.g., developers, operators, organizations). Create legal and ethical guidelines that adapt to evolving AI technologies while protecting human rights.\n",
      "\n",
      "### Implementation Considerations\n",
      "\n",
      "- **Stakeholder Involvement**: Include diverse stakeholders in the design and continuous reevaluation of the ethical framework, ensuring representation from ethicists, legal experts, healthcare professionals, and affected communities.\n",
      "- **Adaptive Policy**: The framework should be flexible enough to adapt to new ethical dilemmas presented by advances in AI and changing societal values.\n",
      "- **Training and Education**: Ensure that developers and operators of AI systems are educated on ethical principles and dilemmas related to life-or-death decision-making.\n",
      "\n",
      "This ethical framework would lay the groundwork for thoughtful, responsible AI decision-making in critical situations while prioritizing accountability and societal trust in the technology.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "Designing an ethical framework for autonomous AI decision-making in life-or-death situations requires careful consideration of various moral principles. Here's a proposed framework that prioritizes the following key principles:\n",
      "\n",
      "1. **Principle of Benefit**: The primary goal is to maximize overall well-being, safety, and survival benefits for individuals and society as a whole.\n",
      "2. **Principle of Non-Maleficence**: Minimize harm and avoid causing unnecessary suffering or injury to any individual or group.\n",
      "3. **Principle of Autonomy**: Respect the autonomy of individuals and allow them to make decisions about their own lives, whenever possible.\n",
      "4. **Principle of Fairness**: Ensure that AI decision-making is fair, transparent, and unbiased, avoiding discrimination and prejudice based on factors like race, gender, age, or social status.\n",
      "5. **Principle of Responsibility**: Hold entities accountable for the consequences of their actions, including AI systems.\n",
      "\n",
      "To reconcile potential conflicts between these principles, consider the following approaches:\n",
      "\n",
      "1. **Gradation of Principles**: Introduce a hierarchical structure to prioritize these principles in different situations, with flexibility to adjust based on context and specific circumstances.\n",
      "2. **Multi-Objective Optimization**: Use machine learning algorithms that optimize multiple objectives simultaneously, allowing AI systems to balance competing values in dynamic environments.\n",
      "3. **Value Alignment**: Incorporate human values into the AI system's decision-making process through value alignment techniques, such as attention-weighted networks, which can help minimize harm and maximize benefits.\n",
      "4. **Regulatory Oversight**: Establish regulatory frameworks that establish clear guidelines for AI development and deployment, ensuring accountability for decisions made by autonomous systems.\n",
      "5. **Transparency and Explainability**: Implement transparent decision-making processes, using techniques like model interpretability or saliency maps to provide insights into the reasoning behind AI-driven decisions.\n",
      "\n",
      "To ensure these principles are consistently applied\n",
      "\n",
      "1.  Establish a clear set of objectives that reflect human values and societal norms.\n",
      "2.  Use value-based frameworks and ontologies for structuring and prioritizing decision-making processes in AI applications.\n",
      "3.  Incorporate diverse perspectives from experts like ethicists, philosophers, and domain experts to develop robust ethical guidelines.\n",
      "\n",
      "These principles can be applied to a range of situations involving autonomous AI, such as:\n",
      "\n",
      "1. **Medical Diagnosis**: Ensure AI systems provide accurate diagnoses that prioritize patient safety, while respecting patient autonomy in informed consent decisions.\n",
      "2.  Autonomous Driving\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"1\", \"2\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gpt-4o-mini\n",
      "Rank 2: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
