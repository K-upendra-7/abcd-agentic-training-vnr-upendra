{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447f6137",
   "metadata": {},
   "source": [
    "# AI Textbook Notes Generator ‚Äî Corrected Notebook\n",
    "\n",
    "This notebook contains a corrected, usable version of your **AI Textbook Notes Generator** application converted into a runnable Python script in notebook form.\n",
    "- I fixed common errors and made the file-handling, chunking, and evaluation parsing more robust.\n",
    "- This notebook still **expects** Ollama (or a compatible local LLM endpoint) and a Google Gemini-compatible API (if you want to run the evaluation / summary steps). If you don't have them, the code will gracefully fallback with informative messages.\n",
    "- Before running cells that call external APIs, set environment variables `OPENAI_API_KEY` and `GOOGLE_API_KEY` (or adapt the code for your particular LLM endpoints).\n",
    "- The notebook saves an executable single-file script and a Gradio UI; you can run it locally in an environment that has the required packages.\n",
    "\n",
    "**Generated on:** 2025-10-29 14:29:17Z (UTC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b773d315",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 496 (3677315812.py, line 497)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 497\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgr.HTML(f\"<div style='text-align:center'><h2>{APP_NAME} v{VERSION}</h2></div>\")\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'with' statement on line 496\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Corrected single-file program (adapted from user's script)\n",
    "# Save and run this in an environment with required packages installed.\n",
    "# Notes on packages:\n",
    "#   pip install gradio PyMuPDF openai python-dotenv fpdf nbformat\n",
    "\n",
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "import textwrap\n",
    "import asyncio\n",
    "import fitz  # PyMuPDF\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel\n",
    "from fpdf import FPDF\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "# Version info\n",
    "VERSION = \"4.0-fixed\"\n",
    "APP_NAME = \"AI Textbook Notes Generator (Complete) - Notebook\"\n",
    "\n",
    "# -----------------\n",
    "# ENVIRONMENT SETUP\n",
    "# -----------------\n",
    "def load_environment():\n",
    "    \"\"\"Load and validate environment configuration\"\"\"\n",
    "    load_dotenv(override=True)\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    # Return booleans and keys (keys may be None)\n",
    "    if not openai_api_key:\n",
    "        print(\"‚ùå OPENAI_API_KEY not found in environment\")\n",
    "    else:\n",
    "        print(f\"‚úÖ OPENAI_API_KEY found: {openai_api_key[:8]}...\")\n",
    "    if not google_api_key:\n",
    "        print(\"‚ùå GOOGLE_API_KEY not found in environment\")\n",
    "    else:\n",
    "        print(f\"‚úÖ GOOGLE_API_KEY found: {google_api_key[:8]}...\")\n",
    "    ok = bool(openai_api_key and google_api_key)\n",
    "    return ok, openai_api_key, google_api_key\n",
    "\n",
    "# -----------------\n",
    "# API CLIENT SETUP\n",
    "# -----------------\n",
    "def setup_api_clients(google_api_key=None):\n",
    "    \"\"\"\n",
    "    Attempt to setup two clients:\n",
    "      - a local LLM (Ollama-like) using AsyncOpenAI pointing at localhost (if available)\n",
    "      - a Gemini/OpenAI-like client using provided google_api_key (if provided)\n",
    "    The code is defensive: it returns None for unavailable clients but does not crash.\n",
    "    \"\"\"\n",
    "    ollama_client = None\n",
    "    gemini_client = None\n",
    "    try:\n",
    "        # Local Ollama-like endpoint if running Ollama with an OpenAI-compatible path\n",
    "        # If you don't have this, these clients remain None and the code will fallback.\n",
    "        ollama_client = AsyncOpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "        print(\"‚úÖ Created Ollama-like AsyncOpenAI client (local) - connection not tested yet.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not create Ollama client: {e}\")\n",
    "\n",
    "    try:\n",
    "        if google_api_key:\n",
    "            gemini_client = AsyncOpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "            print(\"‚úÖ Created Gemini/OpenAI-like client (configured).\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No google_api_key provided ‚Äî Gemini client not configured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not create Gemini client: {e}\")\n",
    "\n",
    "    return ollama_client, gemini_client\n",
    "\n",
    "# -----------------\n",
    "# DATA MODELS\n",
    "# -----------------\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n",
    "    clarity_score: int\n",
    "    accuracy_score: int\n",
    "\n",
    "# -----------------\n",
    "# UTILITIES\n",
    "# -----------------\n",
    "def chunk_text(text: str, max_chars: int = 2500):\n",
    "    \"\"\"Split text into chunks of roughly max_chars length without breaking words.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + max_chars, n)\n",
    "        if end < n:\n",
    "            # try to roll back to nearest newline or space\n",
    "            roll = text.rfind(\"\\n\", start, end)\n",
    "            if roll == -1:\n",
    "                roll = text.rfind(\" \", start, end)\n",
    "            if roll != -1 and roll > start:\n",
    "                end = roll\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "def validate_file_upload(file):\n",
    "    \"\"\"\n",
    "    Accepts different forms returned by Gradio File:\n",
    "      - None\n",
    "      - a dict-like object with 'name' or 'tmp_path' keys (gradio uploads)\n",
    "      - a direct path string or Path object\n",
    "      - a file-like object with .name attribute\n",
    "    Returns (filepath, error_message)\n",
    "    \"\"\"\n",
    "    if file is None:\n",
    "        return None, \"‚ùå No file uploaded. Please select a PDF file.\"\n",
    "\n",
    "    # Handle dict from gradio (it may have 'name' and 'tmp_path')\n",
    "    if isinstance(file, dict):\n",
    "        # For gradio v3+, file can be {'name': '...', 'size':..., 'tmp_path': '/tmp/xxx'}\n",
    "        file_path = file.get(\"tmp_path\") or file.get(\"name\")\n",
    "    elif hasattr(file, \"name\") and os.path.exists(file.name):\n",
    "        file_path = file.name\n",
    "    else:\n",
    "        # Could be a string path\n",
    "        file_path = str(file)\n",
    "\n",
    "    if not file_path:\n",
    "        return None, \"‚ùå Unable to determine file path from upload.\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        return None, f\"‚ùå File not found: {file_path}\"\n",
    "\n",
    "    if not file_path.lower().endswith(\".pdf\"):\n",
    "        return None, f\"‚ùå File must be a PDF. Received: {file_path}\"\n",
    "\n",
    "    try:\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        if file_size == 0:\n",
    "            return None, \"‚ùå File is empty.\"\n",
    "        if file_size > 50 * 1024 * 1024:\n",
    "            return None, \"‚ùå File too large. Maximum size is 50MB.\"\n",
    "    except OSError:\n",
    "        # Could not get size; ignore\n",
    "        pass\n",
    "\n",
    "    return file_path, None\n",
    "\n",
    "# -----------------\n",
    "# PDF TEXT EXTRACTION\n",
    "# -----------------\n",
    "async def extract_text_from_pdf(file_path, progress_callback=None):\n",
    "    if progress_callback:\n",
    "        progress_callback(0.1, \"Initializing PDF extraction...\")\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        num_pages = len(doc)\n",
    "        if num_pages == 0:\n",
    "            return None, \"‚ùå PDF appears to be empty or corrupted.\"\n",
    "        full_text = []\n",
    "        for i, page in enumerate(doc):\n",
    "            if progress_callback:\n",
    "                progress = 0.1 + (i / max(1, num_pages)) * 0.3\n",
    "                progress_callback(progress, f\"Extracting from Page {i+1}/{num_pages}\")\n",
    "            try:\n",
    "                page_text = page.get_text()\n",
    "            except Exception:\n",
    "                page_text = \"\"\n",
    "            if page_text and page_text.strip():\n",
    "                full_text.append(page_text.strip())\n",
    "        doc.close()\n",
    "        combined = \"\\n\\n\".join(full_text).strip()\n",
    "        if not combined:\n",
    "            return None, \"‚ùå No readable text found in PDF. Ensure the PDF contains extractable text.\"\n",
    "        if progress_callback:\n",
    "            progress_callback(0.4, f\"Text extraction complete. {len(combined)} characters found.\")\n",
    "        return combined, None\n",
    "    except Exception as e:\n",
    "        return None, f\"‚ùå Error extracting text from PDF: {e}\"\n",
    "\n",
    "# -----------------\n",
    "# AI NOTE GENERATION (defensive)\n",
    "# -----------------\n",
    "async def generate_notes_with_retry(ollama_client, text_chunk: str, retries: int = 2, feedback: str = \"\"):\n",
    "    system_prompt = (\n",
    "        \"You are an expert academic assistant. Read the provided text and produce well-organized, clear Markdown notes. \"\n",
    "        \"Focus on key concepts, definitions, and main ideas. Keep the language simple but precise. Structure the notes with clear headings and bullet points.\"\n",
    "    )\n",
    "    if feedback:\n",
    "        user_prompt = f\"The previous notes were not acceptable. Improve them using this feedback:\\n{feedback}\\n\\nOriginal Text:\\n{text_chunk}\"\n",
    "    else:\n",
    "        user_prompt = f\"Generate concise academic notes for the following text:\\n{text_chunk}\"\n",
    "\n",
    "    messages = [{\"role\":\"system\",\"content\":system_prompt},{\"role\":\"user\",\"content\":user_prompt}]\n",
    "\n",
    "    # If no ollama_client, return fallback short notes\n",
    "    if not ollama_client:\n",
    "        # fallback simple rule-based extraction: first 3 sentences as a quick 'note'\n",
    "        sent = re.split(r'(?<=[.!?])\\s+', text_chunk.strip())\n",
    "        fallback = \"\\n\\n\".join(sent[:5]) if sent else text_chunk[:500]\n",
    "        return \"# Quick Notes (fallback)\\n\\n\" + fallback\n",
    "\n",
    "    try:\n",
    "        # Attempt to use AsyncOpenAI chat completion interface\n",
    "        response = await ollama_client.chat.completions.create(\n",
    "            model=\"llama3:8b\",\n",
    "            messages=messages,\n",
    "            timeout=60\n",
    "        )\n",
    "        # Some clients return different shapes; guard for content path\n",
    "        content = None\n",
    "        try:\n",
    "            content = response.choices[0].message.content\n",
    "        except Exception:\n",
    "            # try alternative\n",
    "            content = getattr(response, \"content\", None) or str(response)\n",
    "        return content or \"[No content returned from model]\"\n",
    "    except Exception as e:\n",
    "        # On error, return a diagnostic message but keep pipeline moving\n",
    "        return f\"[Model generation error: {e}]\"\n",
    "\n",
    "async def evaluate_notes_quality(gemini_client, text_chunk: str, notes: str) -> Evaluation:\n",
    "    prompt = (\n",
    "        \"You are a strict quality assurance evaluator. Assess the provided notes \"\n",
    "        \"based on their accuracy (do they match the original text?), clarity (are they easy to understand?), and completeness (did they miss key concepts?). \"\n",
    "        \"Return a JSON object with keys: is_acceptable (boolean), feedback (string), clarity_score (int 1-5), accuracy_score (int 1-5).\\n\\n\"\n",
    "        f\"--- Original Text ---\\n{text_chunk}\\n\\n--- Notes ---\\n{notes}\"\n",
    "    )\n",
    "\n",
    "    # If no gemini_client, produce a permissive default evaluation\n",
    "    if not gemini_client:\n",
    "        return Evaluation(is_acceptable=True, feedback=\"No evaluator available (fallback accepted).\", clarity_score=4, accuracy_score=4)\n",
    "\n",
    "    try:\n",
    "        response = await gemini_client.chat.completions.create(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            timeout=30\n",
    "        )\n",
    "        # Extract content and try to parse JSON inside\n",
    "        raw = None\n",
    "        try:\n",
    "            raw = response.choices[0].message.content\n",
    "        except Exception:\n",
    "            raw = getattr(response, \"content\", None) or str(response)\n",
    "        # Try to find a JSON blob in the string\n",
    "        m = re.search(r'\\{.*\\}', raw, flags=re.S)\n",
    "        if m:\n",
    "            json_text = m.group(0)\n",
    "            data = json.loads(json_text)\n",
    "        else:\n",
    "            # fallback simple eval: mark acceptable if notes length > small threshold\n",
    "            data = {\n",
    "                \"is_acceptable\": True if len(notes) > 100 else False,\n",
    "                \"feedback\": \"Parsed evaluation not found ‚Äî applied heuristic fallback.\",\n",
    "                \"clarity_score\": 4,\n",
    "                \"accuracy_score\": 4\n",
    "            }\n",
    "        return Evaluation(**data)\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"‚ö†Ô∏è  Note evaluation timeout - accepting current quality\")\n",
    "        return Evaluation(is_acceptable=True, feedback=\"Evaluation timeout - accepted current quality\", clarity_score=4, accuracy_score=4)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Note evaluation error: {e}\")\n",
    "        return Evaluation(is_acceptable=True, feedback=f\"Evaluation failed: {e}\", clarity_score=4, accuracy_score=4)\n",
    "\n",
    "async def generate_final_summary(gemini_client, all_notes: str):\n",
    "    prompt = (\n",
    "        \"You are an expert academic summarizer. Read all the provided notes and generate a concise, high-level executive summary in clean Markdown.\\n\\n\"\n",
    "        f\"--- All Notes ---\\n{all_notes}\"\n",
    "    )\n",
    "    if not gemini_client:\n",
    "        # Simple heuristic summary fallback\n",
    "        first = all_notes[:2000]\n",
    "        return \"# Executive Summary (fallback)\\n\\n\" + first + \"\\n\\n[Full summary unavailable because no Gemini client configured.]\"\n",
    "    try:\n",
    "        response = await gemini_client.chat.completions.create(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            timeout=45\n",
    "        )\n",
    "        try:\n",
    "            return response.choices[0].message.content\n",
    "        except Exception:\n",
    "            return getattr(response, \"content\", str(response))\n",
    "    except asyncio.TimeoutError:\n",
    "        return \"Executive Summary (Generation timeout - providing basic summary). This textbook covers important concepts.\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Executive summary error: {e}\")\n",
    "        return f\"Executive Summary (Generation failed: {e}).\"\n",
    "\n",
    "# -----------------\n",
    "# PDF CREATION (FPDF)\n",
    "# -----------------\n",
    "class StyledPDF(FPDF):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'I', 8)\n",
    "        self.set_text_color(128,128,128)\n",
    "        self.cell(0,10, APP_NAME, 0, 0, 'C')\n",
    "        self.ln(5)\n",
    "\n",
    "    def footer(self):\n",
    "        self.set_y(-15)\n",
    "        self.set_font('Arial', 'I', 8)\n",
    "        self.set_text_color(128,128,128)\n",
    "        self.cell(0,10, f'Page {self.page_no()}', 0, 0, 'C')\n",
    "\n",
    "    def create_title_page(self, title: str):\n",
    "        self.add_page()\n",
    "        self.set_font('Arial', 'B', 24)\n",
    "        self.set_text_color(0, 51, 102)\n",
    "        self.ln(60)\n",
    "        self.multi_cell(0, 10, title, align='C')\n",
    "        self.ln(10)\n",
    "        self.set_font('Arial', 'I', 14)\n",
    "        self.set_text_color(102,102,102)\n",
    "        self.multi_cell(0, 8, \"Generated by AI Textbook Notes Generator\", align='C')\n",
    "        self.ln(10)\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.set_text_color(128,128,128)\n",
    "        date_str = datetime.now().strftime(\"%B %d, %Y\")\n",
    "        self.multi_cell(0, 8, f\"Generated on {date_str}\", align='C')\n",
    "        self.ln(20)\n",
    "        self.set_line_width(2)\n",
    "        self.set_draw_color(0,51,102)\n",
    "        y = self.get_y()\n",
    "        self.line(15, y, 195, y)\n",
    "\n",
    "    def split_text(self, text, max_width):\n",
    "        words = text.split()\n",
    "        lines = []\n",
    "        current_line = \"\"\n",
    "        for word in words:\n",
    "            test_line = current_line + (\" \" if current_line else \"\") + word\n",
    "            if self.get_string_width(test_line) <= max_width:\n",
    "                current_line = test_line\n",
    "            else:\n",
    "                if current_line:\n",
    "                    lines.append(current_line)\n",
    "                current_line = word\n",
    "        if current_line:\n",
    "            lines.append(current_line)\n",
    "        return lines\n",
    "\n",
    "    def add_section_header(self, title: str):\n",
    "        self.ln(8)\n",
    "        self.set_font('Arial', 'B', 16)\n",
    "        self.set_text_color(0,51,102)\n",
    "        self.cell(0, 12, title, 0, 1, 'L')\n",
    "        self.set_line_width(0.8)\n",
    "        self.set_draw_color(0,51,102)\n",
    "        self.line(15, self.get_y(), 100, self.get_y())\n",
    "        self.ln(6)\n",
    "\n",
    "    def add_content_line(self, text: str, indent: int = 0):\n",
    "        self.set_font('Arial', '', 11)\n",
    "        self.set_text_color(0,0,0)\n",
    "        if text.strip().startswith('‚Ä¢') or text.strip().startswith('-'):\n",
    "            bullet = text.strip()[0]\n",
    "            content = text.strip()[1:].strip()\n",
    "            if indent > 0:\n",
    "                self.cell(indent,6,'',0,0,'L')\n",
    "            self.cell(6,6,bullet,0,0,'L')\n",
    "            lines = self.split_text(content, 180 - indent - 10)\n",
    "            if lines:\n",
    "                self.cell(0,6,lines[0],0,1,'L')\n",
    "                for line in lines[1:]:\n",
    "                    if indent > 0:\n",
    "                        self.cell(indent,6,'',0,0,'L')\n",
    "                    self.cell(6,6,'',0,0,'L')\n",
    "                    self.cell(0,6,line,0,1,'L')\n",
    "        else:\n",
    "            lines = self.split_text(text, 180 - indent)\n",
    "            for i, line in enumerate(lines):\n",
    "                if indent > 0 and i == 0:\n",
    "                    self.cell(indent,6,'',0,0,'L')\n",
    "                self.cell(0,6,line,0,1,'L')\n",
    "\n",
    "    def process_markdown(self, markdown_content: str):\n",
    "        lines = markdown_content.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                self.ln(4)\n",
    "                continue\n",
    "            if line.startswith('# '):\n",
    "                title = line[2:].strip()\n",
    "                self.add_section_header(title)\n",
    "            elif line.startswith('## '):\n",
    "                title = line[3:].strip()\n",
    "                self.set_font('Arial','B',14)\n",
    "                self.set_text_color(51,102,153)\n",
    "                self.ln(4)\n",
    "                self.multi_cell(0,8,title)\n",
    "                self.set_line_width(0.5)\n",
    "                self.set_draw_color(200,200,200)\n",
    "                self.line(15, self.get_y(), 100, self.get_y())\n",
    "                self.ln(6)\n",
    "            elif line.startswith('- ') or line.startswith('‚Ä¢ '):\n",
    "                content = line[2:].strip()\n",
    "                self.add_content_line('‚Ä¢ ' + content, indent=15)\n",
    "            else:\n",
    "                self.add_content_line(line)\n",
    "\n",
    "def create_styled_pdf(notes_markdown: str, source_filename: str) -> tuple:\n",
    "    try:\n",
    "        base_name = os.path.splitext(os.path.basename(source_filename))[0]\n",
    "        output_filename = f\"{base_name}_ai_notes.pdf\"\n",
    "        pdf = StyledPDF()\n",
    "        title = base_name.replace('_',' ').title()\n",
    "        pdf.create_title_page(title)\n",
    "        pdf.process_markdown(notes_markdown)\n",
    "        pdf.output(output_filename)\n",
    "        print(f\"‚úÖ PDF created successfully: {output_filename}\")\n",
    "        return output_filename, None\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        return None, f\"‚ùå Error creating PDF: {e}\\n{tb}\"\n",
    "\n",
    "# -----------------\n",
    "# MAIN PIPELINE\n",
    "# -----------------\n",
    "async def process_textbook_complete(file, progress_callback=None):\n",
    "    if progress_callback:\n",
    "        progress_callback(0.05, \"Validating file...\")\n",
    "    file_path, error = validate_file_upload(file)\n",
    "    if error:\n",
    "        return None, error\n",
    "    if progress_callback:\n",
    "        progress_callback(0.1, \"File validated successfully\")\n",
    "    env_ok, openai_key, google_key = load_environment()\n",
    "    if not env_ok:\n",
    "        # don't fail hard: continue but warn user\n",
    "        print(\"‚ö†Ô∏è  Environment not fully configured - continuing with fallbacks\")\n",
    "    if progress_callback:\n",
    "        progress_callback(0.15, \"Setting up API clients...\")\n",
    "    ollama_client, gemini_client = setup_api_clients(google_key)\n",
    "    if progress_callback:\n",
    "        progress_callback(0.2, \"API clients ready (or fallbacks configured)\")\n",
    "    if progress_callback:\n",
    "        progress_callback(0.4, \"Extracting text from PDF...\")\n",
    "    full_text, error = await extract_text_from_pdf(file_path, progress_callback)\n",
    "    if error:\n",
    "        return None, error\n",
    "    if progress_callback:\n",
    "        progress_callback(0.45, \"Starting note generation...\")\n",
    "    chunks = chunk_text(full_text)\n",
    "    all_notes = []\n",
    "    num_chunks = len(chunks) or 1\n",
    "    if progress_callback:\n",
    "        progress_callback(0.5, f\"Processing {len(chunks)} text chunks...\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if progress_callback:\n",
    "            progress = 0.5 + (i / num_chunks) * 0.3\n",
    "            progress_callback(progress, f\"Generating notes - Chunk {i+1}/{num_chunks}\")\n",
    "        try:\n",
    "            notes_chunk = await generate_notes_with_retry(ollama_client, chunk)\n",
    "            evaluation = await evaluate_notes_quality(gemini_client, chunk, notes_chunk)\n",
    "            if not evaluation.is_acceptable or evaluation.clarity_score < 3 or evaluation.accuracy_score < 3:\n",
    "                print(f\"üîÑ Retrying chunk {i+1} with feedback: {evaluation.feedback}\")\n",
    "                notes_chunk = await generate_notes_with_retry(ollama_client, chunk, retries=1, feedback=evaluation.feedback)\n",
    "            all_notes.append(notes_chunk)\n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc()\n",
    "            print(f\"‚ö†Ô∏è  Error processing chunk {i+1}: {e}\\n{tb}\")\n",
    "            all_notes.append(f\"[Note: Error processing this section - {str(e)}]\")\n",
    "    if progress_callback:\n",
    "        progress_callback(0.8, \"Note generation complete\")\n",
    "    combined_notes = \"\\n\\n---\\n\\n\".join(all_notes)\n",
    "    if progress_callback:\n",
    "        progress_callback(0.82, \"Generating executive summary...\")\n",
    "    final_summary = await generate_final_summary(gemini_client, combined_notes)\n",
    "    if progress_callback:\n",
    "        progress_callback(0.88, \"Executive summary complete\")\n",
    "    final_markdown = f\"# Executive Summary\\n\\n{final_summary}\\n\\n---\\n\\n# Detailed Notes\\n\\n{combined_notes}\"\n",
    "    if progress_callback:\n",
    "        progress_callback(0.9, \"Creating beautiful PDF...\")\n",
    "    pdf_path, error = create_styled_pdf(final_markdown, file_path)\n",
    "    if error:\n",
    "        return None, error\n",
    "    if progress_callback:\n",
    "        progress_callback(1.0, \"Processing complete!\")\n",
    "    return pdf_path, None\n",
    "\n",
    "# ---------------\n",
    "# GRADIO UI\n",
    "# ---------------\n",
    "def create_interface():\n",
    "    with gr.Blocks(css=css) as interface:\n",
    "    gr.HTML(f\"<div style='text-align:center'><h2>{APP_NAME} v{VERSION}</h2></div>\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            file_input = gr.File(label='Upload PDF', file_types=['.pdf'])\n",
    "            gr.Markdown(\"Requirements: PDF must contain extractable text. Max 50MB.\")\n",
    "        with gr.Column(scale=1):\n",
    "            output_file = gr.File(label='Download Generated Notes (.pdf)')\n",
    "    submit_btn = gr.Button('Generate Beautiful Notes')\n",
    "    clear_btn = gr.Button('Clear')\n",
    "    status_output = gr.Textbox(\n",
    "        label='Current Status',\n",
    "        value='Ready to process your textbook',\n",
    "        interactive=False,\n",
    "        lines=2\n",
    "    )\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn=process_with_progress,\n",
    "        inputs=[file_input],\n",
    "        outputs=[output_file]\n",
    "    ).then(\n",
    "        fn=lambda: \"‚úÖ Processing complete! Download your notes.\",\n",
    "        outputs=[status_output]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda: (None, \"Ready to process your textbook\"),\n",
    "        outputs=[output_file, status_output]\n",
    "    )\n",
    "return interface\n",
    "\n",
    "\n",
    "# ---------------\n",
    "# DIAGNOSTICS\n",
    "# ---------------\n",
    "async def run_system_diagnostics():\n",
    "    print(\"ü©∫ Running System Diagnostics...\")\n",
    "    issues = []\n",
    "    print(f\"‚úÖ Python version: {sys.version}\")\n",
    "    required_packages = ['gradio', 'fitz', 'openai', 'pydantic', 'fpdf', 'python_dotenv']\n",
    "    for pkg in required_packages:\n",
    "        try:\n",
    "            __import__(pkg if pkg != 'python_dotenv' else 'dotenv')\n",
    "            print(f\"‚úÖ {pkg} - available\")\n",
    "        except ImportError:\n",
    "            print(f\"‚ùå {pkg} - MISSING\")\n",
    "            issues.append(f\"Install {pkg}: pip install {pkg}\")\n",
    "    env_ok, openai_key, google_key = load_environment()\n",
    "    if not env_ok:\n",
    "        issues.append(\"Configure OPENAI_API_KEY and GOOGLE_API_KEY in .env file\")\n",
    "    # Test Ollama lightly (non-blocking)\n",
    "    try:\n",
    "        ollama, gemini = setup_api_clients(google_key)\n",
    "        if ollama:\n",
    "            try:\n",
    "                # attempt small ping (may fail depending on server)\n",
    "                resp = await ollama.chat.completions.create(model='llama3:8b', messages=[{'role':'user','content':'hi'}], max_tokens=2, timeout=5)\n",
    "                print(\"‚úÖ Ollama-like service - reachable\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Ollama reachable but ping failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Ollama check failed: {e}\")\n",
    "    if not issues:\n",
    "        print(\\\"üéâ All checks passed! System ready to use.\\\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\\\"‚ö†Ô∏è  Issues found:\\\")\n",
    "        for i, issue in enumerate(issues,1):\n",
    "            print(f\\\"  {i}. {issue}\\\")\n",
    "        return False\n",
    "\n",
    "# ---------------\n",
    "# MAIN\n",
    "# ---------------\n",
    "def main():\n",
    "    print(f\\\"üöÄ {APP_NAME} v{VERSION}\\\")\n",
    "    try:\n",
    "        ready = asyncio.run(run_system_diagnostics())\n",
    "        if not ready:\n",
    "            print(\\\"‚ö†Ô∏è  Diagnostics found issues - you can still try running the interface.\\\")\n",
    "    except Exception as e:\n",
    "        print(f\\\"Diagnostics run failed: {e}\\\")\n",
    "    try:\n",
    "        iface = create_interface()\n",
    "        iface.launch(server_name='127.0.0.1', share=False)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\\\"Shutting down...\\\")\n",
    "    except Exception as e:\n",
    "        print(f\\\"Failed to start interface: {e}\\\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
